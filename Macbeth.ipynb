{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kushal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import randint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/kushal/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_text = nltk.corpus.gutenberg.raw('shakespeare-macbeth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through \n"
     ]
    }
   ],
   "source": [
    "print(macbeth_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tragedie of macbeth by william shakespeare actus primus scoena prima thunder and lightning enter three witches when shall we three meet againe in thunder lightning or in raine when the hurley burley done when the battaile lost and wonne that will be ere the set of sunne where the place vpon the heath there to meet with macbeth come gray malkin all padock calls anon faire is foule and foule is faire houer through the fogge and filthie ayre exeunt scena secunda alarum within enter king malcome\n"
     ]
    }
   ],
   "source": [
    "temp = clean_text(macbeth_text)\n",
    "macbeth_text_processed = temp[1:]\n",
    "print(macbeth_text_processed[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 17250 words and 3436 unique words\n"
     ]
    }
   ],
   "source": [
    "macbeth_word_tokenized = word_tokenize(macbeth_text_processed)\n",
    "print(\"The text has %i words and %i unique words\"%(len(macbeth_word_tokenized), len(set(macbeth_word_tokenized))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words = len(set(macbeth_word_tokenized)))\n",
    "tokenizer.fit_on_texts(macbeth_word_tokenized)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "word_2_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparisons\n",
      "1456\n"
     ]
    }
   ],
   "source": [
    "print(macbeth_word_tokenized[500])\n",
    "print(word_2_index[macbeth_word_tokenized[500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequence = [] \n",
    "output_sequence = []\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "for i in range(len(macbeth_word_tokenized) - sequence_length):\n",
    "    temp_input = macbeth_word_tokenized[i:i+sequence_length]\n",
    "    temp_output = macbeth_word_tokenized[i+sequence_length]\n",
    "    input_sequence.append([word_2_index[j] for j in temp_input])\n",
    "    output_sequence.append([word_2_index[temp_output]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17150, 100, 1)\n",
      "(17150, 3437)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "X = np.reshape(input_sequence, (len(input_sequence), sequence_length, 1))\n",
    "X = X / len(set(macbeth_word_tokenized))\n",
    "y = tf.keras.utils.to_categorical(output_sequence)\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_29 (LSTM)               (None, 100, 800)          2566400   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 100, 800)          5123200   \n",
      "_________________________________________________________________\n",
      "lstm_31 (LSTM)               (None, 800)               5123200   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3437)              2753037   \n",
      "=================================================================\n",
      "Total params: 15,565,837\n",
      "Trainable params: 15,565,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(800, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(LSTM(800, return_sequences = True))\n",
    "model.add(LSTM(800))\n",
    "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
    "model.summary()\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,y, batch_size = 64, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
